%%% converge_paper.tex --- 

%% Author: wkretzsch@gmail.com
%% Version: $Id: converge_paper.tex,v 0.0 2013/05/20 09:54:17 winni Exp$

\documentclass[draft,a4paper]{tufte-handout}
%%\usepackage[debugshow,final]{graphics}

\usepackage{color}

% For comments in blue.
\newcommand{\blue}[1]{\textsf{\textbf{\textcolor{blue}{[#1]}}}}
% For comments in magenta.
\newcommand{\magenta}[1]{\textsf{\textbf{\textcolor{magenta}{[#1]}}}}
% For comments in red
\newcommand{\red}[1]{\textsf{\textbf{\textcolor{red}{[#1]}}}}
\linespread{1.6} % for double spacing


%%\revision$Header: /Users/winni/marchini/converge/paper/converge_paper.tex,v 0.0 2013/05/20 09:54:17 winni Exp$
\morefloats
\begin{document}

%%%%##########################################################################


\section{Materials and Methods}
\subsection{Sequence Alignment Pre-Processing for Imputation}

Sequencing Reads for each of the 9300 samples were aligned to the
human reference genome GRCh37.p5 and stored in BAM format. 
The aligner used was Stampy (v1.0.17) \red{ref} with
BWA (v0.5.9) \red{ref}. 
A Base quality score recalibration table was created for each BAM file
using GATK (v2.3.9), and a
reference SNP and indel list from dbSNP version 137, excluding all
  sites after version 129. 
For each BAM file, GATKlite (v2.2.15) \red{ref} was used to remove the reads that were not
properly mapped and recalibrate base quality scores. This involved removing between 1--5$\%$ of reads per sample.

We used SNPTools to calculate genotype likelihoods (GL) at only those
SNP sites that were found to be polymorphic in the 286 Asian samples
in the 1000 Genomes Phase 1 samples \red{ref}. \red{might want to
  address upfront why we didn't do site detection}. This resulted in
GLs at \red{XXXXXXX} SNPs. 

\subsection{Imputation and Phasing}

It is well established that imputation methods can substantially improve genotype calls from low-coverage sequence data \red{TGP, MaCH, SNPTools}. 
Our experience from the 1000 Genomes Projects is that the most
accurate genotype calls can be obtained by first applying the BEAGLE
method to obtain an initial set of estimated haplotypes. These
haplotypes are then used to initialise a program such as Impute2
\red{ref} or MaCH \red{ref} to further refine the estimates. Using
this strategy on 9300 samples requires a large amount of
computational resources so we investigated more efficient methods.  

\red{describe the BEAGLE experiments that reduced iterations. how you did the testing etc.}

BEAGLE (v3.3.2) was run on chunks containing roughly 2800 sites such
that neighboring chunks shared 400 sites. 
At each chunk, BEAGLE was run for six
iterations on the genotype likelihoods and using all 572 TGP phase 1
Asian haplotypes as a reference panel to generate genotype
probabilities.  
After imputation and phasing, the outside 200 sites of every chunk
were discarded.

\section{Results}







%%%%##########################################################################

\end{document}
